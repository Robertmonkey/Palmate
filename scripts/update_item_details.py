"""Fetch detailed item metadata for Palmate's item cards.

The original version of this script only queried the Palworld.gg items
listing.  Unfortunately Palworld.gg currently exposes data for a small
subset of items (mostly weapons and ammunition).  The majority of the
collectibles, cooking ingredients and crafting materials used by
Palmate's data model do not appear on that list which meant we generated
placeholder entries for more than half of the catalogue.

To fully populate the dataset we now combine two data sources:

* Palworld.gg – still the authoritative source for any items that appear
  on the public items page.  These entries are flagged with
  ``fromPalworld = True``.
* Palworld Fandom – used as a fallback for any remaining Palmate item
  keys.  The wiki contains rich infoboxes with type, price and flavour
  text for the missing items.  These entries are tagged with
  ``fromPalworld = False`` and an explicit ``source`` field so the UI can
  surface proper citations if required.

The resulting dataset is written to two files:

* ``data/item_details.json`` – consumable via ``fetch()`` when the app is
  served over HTTP.
* ``data/item_details_fallback.js`` – exposes the same object on
  ``window.__PALMATE_ITEM_DETAILS__`` so the app still works when loaded
  directly from the filesystem.
"""
from __future__ import annotations

import json
import re
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup, Tag

ROOT = Path(__file__).resolve().parents[1]
DATASET_PATH = ROOT / "data" / "palworld_complete_data_final.json"
JSON_TARGET = ROOT / "data" / "item_details.json"
FALLBACK_TARGET = ROOT / "data" / "item_details_fallback.js"
BANNER = "// Auto-generated by scripts/update_item_details.py. Do not edit manually.\n"
GLOBAL_NAME = "__PALMATE_ITEM_DETAILS__"
BASE_URL = "https://palworld.gg"
LISTING_URL = f"{BASE_URL}/items?perPage=400"
FANDOM_BASE_URL = "https://palworld.fandom.com/wiki/"


def slugify(text: str) -> str:
    """Create a Palmate-style slug from Palworld item names."""

    lowered = text.lower()
    normalised = re.sub(r"[^a-z0-9]+", "_", lowered)
    return normalised.strip("_")


def humanise_key(key: str) -> str:
    """Convert an item key like `high_quality_pal_oil` into a display name."""

    parts = [part for part in key.replace("-", "_").split("_") if part]
    return " ".join(part.capitalize() for part in parts) or key


def absolute_url(src: Optional[str]) -> Optional[str]:
    if not src:
        return None
    normalized = src.strip()
    if not normalized:
        return None
    if normalized.startswith(("http://", "https://")):
        return normalized
    return urljoin(f"{BASE_URL}/", normalized)


ItemDetail = Dict[str, Any]


def parse_item_block(block: Tag) -> Optional[Tuple[ItemDetail, str]]:
    name_el = block.select_one(".up .text")
    if not name_el:
        return None
    name = name_el.get_text(strip=True)
    slug = slugify(name)

    type_el = block.select_one(".up .type")
    item_type = type_el.get_text(strip=True) if type_el else None

    desc_el = block.select_one(".item-card .description")
    description = [
        segment.strip()
        for segment in desc_el.stripped_strings
        if segment and segment.strip()
    ] if desc_el else []

    stats: Dict[str, str] = {}
    for key_el in block.select(".item-card .keys .key"):
        label_el = key_el.select_one(".text")
        value_el = key_el.select_one(".value")
        if not label_el or not value_el:
            continue
        label = label_el.get_text(strip=True)
        value = value_el.get_text(strip=True)
        if value:
            stats[label] = value

    recipe: List[Dict[str, Any]] = []
    for entry in block.select(".item-card .recipe .item"):
        name_el = entry.select_one(".name")
        if not name_el:
            continue
        name_text = name_el.get_text(strip=True).replace('\xa0', ' ')
        quantity: Optional[int] = None
        ingredient = name_text
        match = re.match(r"^(\d+)\s+(.+)$", name_text)
        if match:
            quantity = int(match.group(1))
            ingredient = match.group(2)
        icon_el = entry.select_one(".image img")
        icon_src = absolute_url(icon_el["src"]) if icon_el and icon_el.get("src") else None
        recipe.append({
            "name": ingredient,
            "quantity": quantity,
            "icon": icon_src,
        })

    image_el = block.select_one(".up .image img")
    image = absolute_url(image_el["src"]) if image_el and image_el.get("src") else None

    return {
        "name": name,
        "type": item_type,
        "description": description,
        "stats": stats,
        "recipe": recipe,
        "image": image,
        "fromPalworld": True,
        "source": "Palworld.gg",
    }, slug


def fetch_remote_details() -> Dict[str, ItemDetail]:
    response = requests.get(LISTING_URL, timeout=30)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")
    details: Dict[str, ItemDetail] = {}
    for block in soup.select("div.item"):
        parsed = parse_item_block(block)
        if not parsed:
            continue
        data, slug = parsed
        # Only keep the first occurrence for duplicate names.
        details.setdefault(slug, data)
    return details


def ensure_absolute(url: Optional[str]) -> Optional[str]:
    """Normalise protocol-relative image URLs returned by Fandom."""

    if not url:
        return None
    normalized = url.strip()
    if not normalized:
        return None
    if normalized.startswith(("http://", "https://")):
        return normalized
    if normalized.startswith("//"):
        return f"https:{normalized}"
    return urljoin("https://palworld.fandom.com/", normalized)


def extract_paragraphs(nodes: Iterable[Tag], limit: int = 2) -> List[str]:
    """Collect human-friendly text snippets from paragraph nodes."""

    snippets: List[str] = []
    for node in nodes:
        text = node.get_text(" ", strip=True)
        if not text:
            continue
        lower = text.lower()
        if "information type" in lower:
            continue
        if lower in {"pals", "drops"}:
            continue
        if not any(char.isalpha() for char in text):
            continue
        if "." not in text and " is " not in lower and " are " not in lower:
            continue
        snippets.append(text)
        if len(snippets) >= limit:
            break
    return snippets


def fetch_fandom_detail(item_key: str, fallback_category: str) -> Optional[ItemDetail]:
    """Attempt to fetch item metadata from the Palworld Fandom wiki."""

    human_name = humanise_key(item_key)
    page_slug = human_name.replace(" ", "_")
    url = f"{FANDOM_BASE_URL}{page_slug}"

    try:
        response = requests.get(url, timeout=30)
        if response.status_code == 404:
            return None
        response.raise_for_status()
    except requests.RequestException:
        return None

    soup = BeautifulSoup(response.text, "html.parser")

    title_el = soup.select_one("h1.page-header__title")
    name = title_el.get_text(strip=True) if title_el else human_name

    content = soup.select_one("#mw-content-text")
    summary: List[str] = []
    if content:
        summary = extract_paragraphs(content.select("p"))

    infobox = soup.select_one("aside.portable-infobox")
    stats: Dict[str, str] = {}
    item_type = fallback_category or "Item"
    image: Optional[str] = None

    if infobox:
        image_el = infobox.select_one("figure.pi-image img")
        if image_el and image_el.get("src"):
            image = ensure_absolute(image_el.get("data-src") or image_el["src"])

        for section in infobox.select("section.pi-item"):
            label_el = section.select_one(".pi-data-label")
            value_el = section.select_one(".pi-data-value")
            if not label_el or not value_el:
                continue
            label = label_el.get_text(strip=True)
            value = value_el.get_text(" ", strip=True)
            if not value:
                continue
            if label.lower() == "type":
                item_type = value
            else:
                stats[label] = value

    description = summary or [
        f"{name} is listed as a {item_type or fallback_category} on the Palworld Fandom wiki.",
        "Full Palworld.gg data for this item has not been published yet.",
    ]

    return {
        "name": name,
        "type": item_type or fallback_category or "Item",
        "description": description,
        "stats": stats,
        "recipe": [],
        "image": image,
        "fromPalworld": False,
        "source": "Palworld Fandom",
    }


def build_full_dataset(scraped: Dict[str, ItemDetail]) -> Dict[str, ItemDetail]:
    with DATASET_PATH.open("r", encoding="utf-8") as handle:
        payload = json.load(handle)
    items = payload.get("items", {})
    combined: Dict[str, ItemDetail] = {}

    for key in sorted(items.keys()):
        if key in scraped:
            combined[key] = scraped[key]
            continue
        category = items[key].get("category", "Item")
        fandom_detail = fetch_fandom_detail(key, category)
        if fandom_detail:
            combined[key] = fandom_detail
            continue
        combined[key] = {
            "name": humanise_key(key),
            "type": category,
            "description": [
                f"Detailed data for {humanise_key(key)} is not yet available on Palworld.gg or the Palworld wiki.",
                "These values are based on Palmate's internal category data.",
            ],
            "stats": {},
            "recipe": [],
            "image": None,
            "fromPalworld": False,
            "source": "Palmate",
        }
    return combined


def write_outputs(details: Dict[str, ItemDetail]) -> None:
    json_payload = json.dumps(details, indent=2, ensure_ascii=False)
    JSON_TARGET.write_text(json_payload + "\n", encoding="utf-8")

    js_payload = json.dumps(details, indent=2, ensure_ascii=False)
    fallback_contents = f"{BANNER}window.{GLOBAL_NAME} = {js_payload};\n"
    FALLBACK_TARGET.write_text(fallback_contents, encoding="utf-8")


def main() -> None:
    scraped = fetch_remote_details()
    combined = build_full_dataset(scraped)
    write_outputs(combined)
    palworld_count = sum(1 for detail in combined.values() if detail.get("fromPalworld"))
    fandom_count = sum(1 for detail in combined.values() if detail.get("source") == "Palworld Fandom")
    placeholder_count = len(combined) - palworld_count - fandom_count
    print(
        f"Wrote {len(combined)} item detail entries "
        f"({palworld_count} from Palworld.gg, {fandom_count} from Palworld Fandom, "
        f"{placeholder_count} placeholders)."
    )


if __name__ == "__main__":
    main()
